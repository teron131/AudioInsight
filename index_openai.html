<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Live Audio Transcription</title>
    <style>
        body { font-family: sans-serif; margin: 20px; background-color: #f4f4f4; color: #333; }
        .container { max-width: 800px; margin: auto; background: white; padding: 20px; border-radius: 8px; box-shadow: 0 0 10px rgba(0,0,0,0.1); }
        h1 { text-align: center; color: #4CAF50; }
        #controls { margin-bottom: 20px; text-align: center; }
        button {
            padding: 10px 20px;
            margin: 5px;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            font-size: 16px;
        }
        #startButton { background-color: #4CAF50; color: white; }
        #stopButton { background-color: #f44336; color: white; }
        #uploadButton { background-color: #2196F3; color: white; }
        #updateConfigButton { background-color: #FF9800; color: white; }
        #getConfigButton { background-color: #9C27B0; color: white; }
        #fileInput { margin-right: 10px; padding: 5px; }
        #analysis-config { 
            margin-top: 10px; 
            padding: 10px; 
            background-color: #f0f0f0; 
            border-radius: 5px; 
            font-size: 14px;
        }
        #analysis-config label { font-weight: bold; }
        #analysis-info { font-size: 14px; color: #666; font-weight: normal; }
        
        #transcription-section, #analysis-section {
            margin: 20px 0;
            padding: 15px;
            border: 1px solid #ddd;
            border-radius: 8px;
            background-color: #fafafa;
        }
        
        #transcription-section h3, #analysis-section h3 {
            margin: 0 0 10px 0;
            color: #333;
        }
        
        #transcription, #analysis {
            min-height: 100px;
            max-height: 200px;
            overflow-y: auto;
            padding: 10px;
            background-color: white;
            border: 1px solid #eee;
            border-radius: 4px;
            white-space: pre-wrap;
            font-family: monospace;
            line-height: 1.4;
        }
        #messages {
            height: 300px;
            overflow-y: auto;
            border: 1px solid #ddd;
            padding: 10px;
            margin-bottom: 10px;
            background-color: #fff;
            white-space: pre-wrap; /* Preserve line breaks and spaces */
        }
        .message { margin-bottom: 5px; padding: 8px; border-radius: 4px; }
        .user { background-color: #e1f5fe; text-align: right; }
        .gemini { background-color: #e8f5e9; }
        .error { background-color: #ffebee; color: #c62828; }
        .info { background-color: #e3f2fd; color: #1565c0; font-style: italic;}
        #status { text-align: center; font-style: italic; color: #777; margin-bottom: 10px;}
    </style>
</head>
<body>
    <div class="container">
        <h1>Live Audio Transcription & Analysis</h1>
        <div id="controls">
            <button id="startButton">Start Recording</button>
            <button id="stopButton" disabled>Stop Recording</button>
            <br><br>
            <input type="file" id="fileInput" accept="audio/*" />
            <button id="uploadButton">Upload & Analyze Audio File</button>
            <br><br>
            <div id="analysis-config">
                <label for="analysisInterval">Analysis Interval (seconds):</label>
                <input type="number" id="analysisInterval" min="1" max="30" step="0.5" value="5.0" style="width: 60px; margin: 0 10px;">
                <button id="updateConfigButton">Update Config</button>
                <button id="getConfigButton">Get Current Config</button>
            </div>
        </div>
        <div id="status">Status: Idle</div>
        
        <div id="transcription-section">
            <h3>ðŸŽ¤ Live Transcription</h3>
            <div id="transcription"></div>
        </div>
        
        <div id="analysis-section">
            <h3>ðŸ¤– AI Analysis <span id="analysis-info">(Structured Output)</span></h3>
            <div id="analysis"></div>
            <div id="analysis-config-display" style="margin-top: 10px; font-size: 12px; color: #666;">
                <strong>Current Config:</strong> <span id="config-status">Loading...</span>
            </div>
        </div>
        
        <div id="messages"></div>
    </div>

    <script>
        const startButton = document.getElementById('startButton');
        const stopButton = document.getElementById('stopButton');
        const messagesDiv = document.getElementById('messages');
        const statusDiv = document.getElementById('status');
        const transcriptionDiv = document.getElementById('transcription');
        const analysisDiv = document.getElementById('analysis');
        
        // Transcription accumulation
        let transcriptionBuffer = '';
        let transcriptionUpdateTimer = null;
        let currentTranscriptSegments = new Map(); // Track segments by item_id

        let audioContext;
        let mediaStream;
        let scriptProcessor;
        let socket;
        let audioProcessCallCount = 0; // For less frequent logging
        let isRecording = false;
        let reconnectionTimeout = null;

        const TARGET_SAMPLE_RATE = 16000;
        const BUFFER_SIZE = 4096; // Buffer size for ScriptProcessorNode
        const RECONNECTION_DELAY = 1000; // 1 second delay between connections

        function logMessage(text, type = 'gemini') {
            console.log(`[UI Log - ${type}]: ${text}`); // Added console.log
            const messageElement = document.createElement('div');
            messageElement.classList.add('message', type);
            messageElement.textContent = text;
            messagesDiv.appendChild(messageElement);
            messagesDiv.scrollTop = messagesDiv.scrollHeight; // Auto-scroll
        }
        
        function updateStatus(message) {
            console.log(`[UI Status]: ${message}`);
            statusDiv.textContent = `Status: ${message}`;
        }
        
        function updateTranscription(newText) {
            // Add space between segments if buffer is not empty and doesn't end with space
            if (transcriptionBuffer && !transcriptionBuffer.endsWith(' ') && !newText.startsWith(' ')) {
                transcriptionBuffer += ' ';
            }
            transcriptionBuffer += newText;
            
            // Clear existing timer
            if (transcriptionUpdateTimer) {
                clearTimeout(transcriptionUpdateTimer);
            }
            
            // Update after a short delay to accumulate chunks
            transcriptionUpdateTimer = setTimeout(() => {
                transcriptionDiv.textContent = transcriptionBuffer;
                transcriptionDiv.scrollTop = transcriptionDiv.scrollHeight;
            }, 100);
        }
        
        let currentDeltaBuffer = '';
        
        function handleTranscriptionDelta(deltaText) {
            // Accumulate delta text for real-time display
            currentDeltaBuffer += deltaText;
            
            // Update the live transcription section with current progress
            transcriptionDiv.textContent = transcriptionBuffer + (transcriptionBuffer ? ' ' : '') + currentDeltaBuffer;
            transcriptionDiv.scrollTop = transcriptionDiv.scrollHeight;
        }
        
        function handleTranscriptionComplete(completeText) {
            // When transcription is complete, add it to the final buffer
            if (transcriptionBuffer && !transcriptionBuffer.endsWith(' ')) {
                transcriptionBuffer += ' ';
            }
            transcriptionBuffer += completeText;
            
            // Clear the delta buffer since we have the final version
            currentDeltaBuffer = '';
            
            // Update the display with the complete transcription
            transcriptionDiv.textContent = transcriptionBuffer;
            transcriptionDiv.scrollTop = transcriptionDiv.scrollHeight;
            
            console.log(`[UI Transcription]: Complete segment added: "${completeText}"`);
        }
        
        function handleTranscriptionFullTurn(fullTurnText) {
            // For whisper-1 style where delta contains full turn transcript
            // Replace the current delta buffer with the full turn
            currentDeltaBuffer = fullTurnText;
            
            // Update the display with current progress
            transcriptionDiv.textContent = transcriptionBuffer + (transcriptionBuffer ? ' ' : '') + currentDeltaBuffer;
            transcriptionDiv.scrollTop = transcriptionDiv.scrollHeight;
            
            console.log(`[UI Transcription]: Full turn update: "${fullTurnText}"`);
        }
        
        function updateAnalysis(text, isUpdate = false) {
            try {
                // Try to parse as JSON for structured output
                const analysisData = JSON.parse(text);
                
                // Create formatted display for structured data
                const formattedText = formatStructuredAnalysis(analysisData);
                
                // Add with timestamp
                const timestamp = new Date().toLocaleTimeString();
                
                if (isUpdate) {
                    // Replace the content for updates (single summary variable)
                    analysisDiv.textContent = `[${timestamp}] ${formattedText}`;
                } else {
                    // Append for initial or other messages
                    if (analysisDiv.textContent) {
                        analysisDiv.textContent += '\n\n';
                    }
                    analysisDiv.textContent += `[${timestamp}] ${formattedText}`;
                }
                
            } catch (e) {
                // Fallback to plain text if not valid JSON
                if (isUpdate) {
                    analysisDiv.textContent = text;
                } else {
                    if (analysisDiv.textContent && !analysisDiv.textContent.endsWith(' ')) {
                        analysisDiv.textContent += ' ';
                    }
                    analysisDiv.textContent += text;
                }
            }
            analysisDiv.scrollTop = analysisDiv.scrollHeight;
        }
        
        function formatStructuredAnalysis(data) {
            // Format structured analysis data for display
            if (data.summary) {
                return `Summary: ${data.summary}`;
            }
            // Handle other structured fields if they exist
            return JSON.stringify(data, null, 2);
        }
        
        function clearSections() {
            transcriptionBuffer = '';
            currentDeltaBuffer = '';
            transcriptionDiv.textContent = '';
            analysisDiv.textContent = '';
        }

        function floatTo16BitPCM(inputFloat32Array) {
            const output = new Int16Array(inputFloat32Array.length);
            for (let i = 0; i < inputFloat32Array.length; i++) {
                const s = Math.max(-1, Math.min(1, inputFloat32Array[i]));
                output[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
            }
            return output.buffer; // Return ArrayBuffer
        }
        
        // Simple downsampling: take every Nth sample.
        // This is a basic approach; for production, a proper resampler (e.g., in an AudioWorklet) is better.
        function downsampleBuffer(buffer, inputSampleRate, outputSampleRate) {
            if (inputSampleRate === outputSampleRate) {
                return buffer;
            }
            if (inputSampleRate < outputSampleRate) {
                console.warn("[UI Log]: Upsampling not supported by this simple downsampler.");
                return buffer; 
            }
            const sampleRateRatio = inputSampleRate / outputSampleRate;
            const newLength = Math.round(buffer.length / sampleRateRatio);
            const result = new Float32Array(newLength);
            let offsetResult = 0;
            let offsetBuffer = 0;
            while (offsetResult < result.length) {
                const nextOffsetBuffer = Math.round((offsetResult + 1) * sampleRateRatio);
                let accum = 0, count = 0;
                for (let i = offsetBuffer; i < nextOffsetBuffer && i < buffer.length; i++) {
                    accum += buffer[i];
                    count++;
                }
                result[offsetResult] = count > 0 ? accum / count : 0; // Averaging, or simply buffer[Math.floor(offsetResult * sampleRateRatio)];
                // result[offsetResult] = buffer[Math.floor(offsetResult * sampleRateRatio)]; // Simpler picking
                offsetResult++;
                offsetBuffer = nextOffsetBuffer;
            }
            return result;
        }

        async function cleanupAudioResources() {
            console.log("[UI Process]: Cleaning up audio resources...");
            
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                console.log("[UI Process]: MediaStream tracks stopped.");
                mediaStream = null;
            }
            
            if (scriptProcessor) {
                scriptProcessor.disconnect(); 
                scriptProcessor.onaudioprocess = null; 
                console.log("[UI Process]: ScriptProcessorNode disconnected and cleaned up.");
                scriptProcessor = null;
            }
            
            if (audioContext) {
                try {
                    await audioContext.close();
                    console.log("[UI Process]: AudioContext closed.");
                } catch (e) {
                    console.error("[UI Error]: Error closing AudioContext:", e);
                }
                audioContext = null;
            }
        }

        async function cleanupWebSocket() {
            console.log("[UI Process]: Cleaning up WebSocket...");
            
            if (socket) {
                if (socket.readyState === WebSocket.OPEN || socket.readyState === WebSocket.CONNECTING) {
                    console.log("[UI WebSocket]: Closing WebSocket connection.");
                    socket.close(1000, "Client stopped recording");
                }
                socket = null;
            }
            
            // Clear any pending reconnection timeouts
            if (reconnectionTimeout) {
                clearTimeout(reconnectionTimeout);
                reconnectionTimeout = null;
            }
        }

        async function stopRecording() {
            console.log("[UI Action]: Stopping recording...");
            isRecording = false;
            
            // Clean up resources in order
            await cleanupAudioResources();
            await cleanupWebSocket();
            
            startButton.disabled = false;
            stopButton.disabled = true;
            updateStatus('Recording stopped. Ready to start.');
            logMessage('Recording stopped by user.', 'info');
        }

        startButton.onclick = async () => {
            console.log("[UI Action]: Start Recording button clicked.");
            
            // Prevent multiple simultaneous recordings
            if (isRecording) {
                console.log("[UI Process]: Already recording, ignoring start request.");
                return;
            }
            
            audioProcessCallCount = 0; // Reset counter
            isRecording = true;
            clearSections();  // Clear previous results
            
            try {
                updateStatus('Requesting microphone permission...');
                console.log("[UI Process]: Requesting microphone permission...");
                mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                console.log("[UI Process]: Microphone access GRANTED.");
                
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                console.log(`[UI Process]: AudioContext created. State: ${audioContext.state}, SampleRate: ${audioContext.sampleRate}`);
                
                if (audioContext.state === 'suspended') {
                    console.log("[UI Process]: AudioContext is suspended, attempting to resume...");
                    await audioContext.resume();
                    console.log(`[UI Process]: AudioContext resumed. State: ${audioContext.state}`);
                }

                updateStatus('Initializing audio...');
                logMessage('Microphone access granted.', 'info');

                const source = audioContext.createMediaStreamSource(mediaStream);
                console.log("[UI Process]: MediaStreamSource created.");
                scriptProcessor = audioContext.createScriptProcessor(BUFFER_SIZE, 1, 1); 
                console.log("[UI Process]: ScriptProcessorNode created.");

                scriptProcessor.onaudioprocess = (event) => {
                    if (!isRecording) {
                        return; // Stop processing if recording was stopped
                    }
                    
                    audioProcessCallCount++;
                    if (!socket || socket.readyState !== WebSocket.OPEN) {
                        if (audioProcessCallCount % 100 === 1) { // Log occasionally if socket not open
                            console.warn("[UI AudioProcess]: onaudioprocess called but WebSocket is not open. Skipping send.");
                        }
                        return;
                    }
                    const inputData = event.inputBuffer.getChannelData(0); 
                    
                    let processedData = inputData;
                    if (audioContext.sampleRate !== TARGET_SAMPLE_RATE) {
                         processedData = downsampleBuffer(inputData, audioContext.sampleRate, TARGET_SAMPLE_RATE);
                    }
                    
                    const pcmDataBuffer = floatTo16BitPCM(processedData);
                    if (audioProcessCallCount % 50 === 1) { // Log occasionally to show it's working
                        console.log(`[UI AudioProcess]: Sending audio data chunk. Size: ${pcmDataBuffer.byteLength} bytes.`);
                    }
                    socket.send(pcmDataBuffer);
                };

                source.connect(scriptProcessor);
                scriptProcessor.connect(audioContext.destination); 
                console.log("[UI Process]: Audio graph connected (source -> scriptProcessor -> destination).");

                // Add a small delay before connecting WebSocket to ensure clean state
                await new Promise(resolve => setTimeout(resolve, 500));
                await connectWebSocket();

                startButton.disabled = true;
                stopButton.disabled = false;
                updateStatus('Recording...');

            } catch (err) {
                console.error('[UI Error]: Error starting recording:', err);
                logMessage(`Error starting recording: ${err.message}`, 'error');
                updateStatus(`Error: ${err.message}`);
                isRecording = false;
                await stopRecording();
            }
        };

        stopButton.onclick = async () => {
            console.log("[UI Action]: Stop Recording button clicked.");
            await stopRecording();
        };

        // Configuration management
        const analysisIntervalInput = document.getElementById('analysisInterval');
        const updateConfigButton = document.getElementById('updateConfigButton');
        const getConfigButton = document.getElementById('getConfigButton');
        const configStatusSpan = document.getElementById('config-status');

        // Load initial configuration
        loadCurrentConfig();

        async function loadCurrentConfig() {
            try {
                const response = await fetch('http://localhost:8889/api/analysis/config');
                const config = await response.json();
                
                analysisIntervalInput.value = config.analysis_interval_seconds;
                configStatusSpan.textContent = `Model: ${config.analysis_model}, Interval: ${config.analysis_interval_seconds}s, Min Length: ${config.min_transcript_length}`;
                
                console.log('[UI Config]: Loaded current config:', config);
            } catch (error) {
                console.error('[UI Config]: Failed to load config:', error);
                configStatusSpan.textContent = 'Failed to load config';
            }
        }

        updateConfigButton.onclick = async () => {
            try {
                const newInterval = parseFloat(analysisIntervalInput.value);
                
                const response = await fetch('http://localhost:8889/api/analysis/config', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        analysis_interval_seconds: newInterval
                    })
                });
                
                const result = await response.json();
                console.log('[UI Config]: Updated config:', result);
                logMessage(`Config updated: Analysis interval set to ${newInterval}s`, 'info');
                
                // Refresh config display
                await loadCurrentConfig();
                
            } catch (error) {
                console.error('[UI Config]: Failed to update config:', error);
                logMessage('Failed to update configuration', 'error');
            }
        };

        getConfigButton.onclick = loadCurrentConfig;

        // File upload functionality
        const fileInput = document.getElementById('fileInput');
        const uploadButton = document.getElementById('uploadButton');

        uploadButton.onclick = async () => {
            const file = fileInput.files[0];
            if (!file) {
                logMessage('Please select an audio file first.', 'error');
                return;
            }

            console.log(`[UI Action]: Uploading file: ${file.name}`);
            updateStatus('Processing audio file...');
            clearSections();  // Clear previous results
            
            try {
                // Convert file to the required format using Web Audio API
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const arrayBuffer = await file.arrayBuffer();
                const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                
                // Convert to 16kHz mono
                const targetSampleRate = 16000;
                const offlineContext = new OfflineAudioContext(1, audioBuffer.duration * targetSampleRate, targetSampleRate);
                const source = offlineContext.createBufferSource();
                source.buffer = audioBuffer;
                source.connect(offlineContext.destination);
                source.start();
                
                const renderedBuffer = await offlineContext.startRendering();
                const float32Data = renderedBuffer.getChannelData(0);
                
                // Convert to 16-bit PCM
                const pcmData = floatTo16BitPCM(float32Data);
                
                logMessage(`File processed: ${file.name} (${(pcmData.byteLength / 1024).toFixed(1)} KB)`, 'info');
                
                // Send to WebSocket
                await sendAudioFileToServer(pcmData);
                
            } catch (error) {
                console.error('[UI Error]: File processing failed:', error);
                logMessage(`Error processing file: ${error.message}`, 'error');
                updateStatus('Error processing file');
            }
        };

        async function sendAudioFileToServer(pcmData) {
            const wsUrl = 'ws://localhost:8889/ws';
            console.log(`[UI WebSocket]: Connecting to ${wsUrl} for file upload`);
            
            try {
                const socket = new WebSocket(wsUrl);
                
                socket.onopen = async () => {
                    console.log("[UI WebSocket]: Connected for file upload");
                    updateStatus('Connected. Sending audio file...');
                    
                    // Send audio data in chunks
                    const chunkSize = 4096; // 4KB chunks
                    let offset = 0;
                    
                    while (offset < pcmData.byteLength) {
                        const chunk = pcmData.slice(offset, Math.min(offset + chunkSize, pcmData.byteLength));
                        socket.send(chunk);
                        offset += chunkSize;
                        
                        // Small delay to prevent overwhelming the server
                        await new Promise(resolve => setTimeout(resolve, 50));
                    }
                    
                    console.log("[UI WebSocket]: Finished sending file data");
                    updateStatus('Audio sent. Waiting for analysis...');
                };
                
                socket.onmessage = (event) => {
                    const message = event.data;
                    console.log(`[UI WebSocket]: Response: ${message}`);
                    
                    if (message.startsWith("INFO:")) {
                        logMessage(message, 'info');
                    } else if (message.startsWith("ERROR:")) {
                        logMessage(message, 'error');
                    } else if (message.startsWith("TRANSCRIPTION_DELTA:")) {
                        // Handle real-time transcription deltas (incremental)
                        const deltaText = message.replace("TRANSCRIPTION_DELTA:", "").trim();
                        handleTranscriptionDelta(deltaText);
                    } else if (message.startsWith("TRANSCRIPTION_COMPLETE:")) {
                        // Handle completed transcription (incremental models)
                        const completeText = message.replace("TRANSCRIPTION_COMPLETE:", "").trim();
                        handleTranscriptionComplete(completeText);
                    } else if (message.startsWith("TRANSCRIPTION_FULL_TURN:")) {
                        // Handle full turn transcription (whisper-1 style)
                        const fullTurnText = message.replace("TRANSCRIPTION_FULL_TURN:", "").trim();
                        handleTranscriptionFullTurn(fullTurnText);
                    } else if (message.startsWith("ðŸŽ¤ TRANSCRIPTION:")) {
                        // Legacy format support
                        const transcriptionText = message.replace("ðŸŽ¤ TRANSCRIPTION:", "").trim();
                        updateTranscription(transcriptionText);
                    } else if (message.startsWith("ðŸ¤– AI ANALYSIS UPDATE:")) {
                        // Extract AI analysis text and update (replace) the analysis section
                        const analysisText = message.replace("ðŸ¤– AI ANALYSIS UPDATE:", "").trim();
                        updateAnalysis(analysisText, true);
                    } else if (message.startsWith("ðŸ¤– AI ANALYSIS:")) {
                        // Extract AI analysis text and append to the analysis section
                        const analysisText = message.replace("ðŸ¤– AI ANALYSIS:", "").trim();
                        updateAnalysis(analysisText, false);
                    } else {
                        // Fallback for other messages - treat as analysis
                        updateAnalysis(message, false);
                    }
                };
                
                socket.onerror = (error) => {
                    console.error('[UI WebSocket]: Upload error:', error);
                    logMessage('WebSocket error during file upload', 'error');
                    updateStatus('Upload error');
                };
                
                socket.onclose = (event) => {
                    console.log(`[UI WebSocket]: Upload connection closed`);
                    updateStatus('File analysis complete');
                };
                
            } catch (error) {
                console.error('[UI Error]: Upload failed:', error);
                logMessage(`Upload failed: ${error.message}`, 'error');
                updateStatus('Upload failed');
            }
        }

        async function connectWebSocket() {
            const wsUrl = 'ws://localhost:8889/ws';
            console.log(`[UI WebSocket]: Attempting to connect to ${wsUrl}`);
            
            return new Promise((resolve, reject) => {
                socket = new WebSocket(wsUrl); 
                updateStatus('Connecting to server...');

                const connectionTimeout = setTimeout(() => {
                    console.error("[UI WebSocket]: Connection timeout");
                    reject(new Error("WebSocket connection timeout"));
                }, 10000); // 10 second timeout

                socket.onopen = () => {
                    clearTimeout(connectionTimeout);
                    console.log("[UI WebSocket]: Connection OPENED.");
                    logMessage('Connected to server via WebSocket.', 'info');
                    updateStatus('Connected. Recording...');
                    resolve();
                };

                socket.onmessage = (event) => {
                    const message = event.data;
                    console.log(`[UI WebSocket]: Message RECEIVED: ${message}`);
                    
                    if (message.startsWith("INFO:")) {
                        logMessage(message, 'info');
                    } else if (message.startsWith("ERROR:") || message.startsWith("GEMINI_ERROR:") || message.startsWith("SERVER_ERROR_PROCESSING_GEMINI_RESPONSE:")) {
                        logMessage(message, 'error');
                    } else if (message.startsWith("TRANSCRIPTION_DELTA:")) {
                        // Handle real-time transcription deltas (incremental)
                        const deltaText = message.replace("TRANSCRIPTION_DELTA:", "").trim();
                        handleTranscriptionDelta(deltaText);
                    } else if (message.startsWith("TRANSCRIPTION_COMPLETE:")) {
                        // Handle completed transcription (incremental models)
                        const completeText = message.replace("TRANSCRIPTION_COMPLETE:", "").trim();
                        handleTranscriptionComplete(completeText);
                    } else if (message.startsWith("TRANSCRIPTION_FULL_TURN:")) {
                        // Handle full turn transcription (whisper-1 style)
                        const fullTurnText = message.replace("TRANSCRIPTION_FULL_TURN:", "").trim();
                        handleTranscriptionFullTurn(fullTurnText);
                    } else if (message.startsWith("ðŸŽ¤ TRANSCRIPTION:")) {
                        // Legacy format support
                        const transcriptionText = message.replace("ðŸŽ¤ TRANSCRIPTION:", "").trim();
                        updateTranscription(transcriptionText);
                    } else if (message.startsWith("ðŸ¤– AI ANALYSIS UPDATE:")) {
                        // Extract AI analysis text and update (replace) the analysis section
                        const analysisText = message.replace("ðŸ¤– AI ANALYSIS UPDATE:", "").trim();
                        updateAnalysis(analysisText, true);
                    } else if (message.startsWith("ðŸ¤– AI ANALYSIS:")) {
                        // Extract AI analysis text and append to the analysis section
                        const analysisText = message.replace("ðŸ¤– AI ANALYSIS:", "").trim();
                        updateAnalysis(analysisText, false);
                    } else {
                        // Fallback for other messages - treat as analysis
                        updateAnalysis(message, false);
                    }
                };

                socket.onerror = (error) => {
                    clearTimeout(connectionTimeout);
                    console.error('[UI WebSocket]: Error EVENT:', error);
                    logMessage('WebSocket error. See console for details.', 'error');
                    updateStatus('WebSocket error.');
                    reject(error);
                };

                socket.onclose = (event) => {
                    clearTimeout(connectionTimeout);
                    console.log(`[UI WebSocket]: Connection CLOSED. Code: ${event.code}, Reason: '${event.reason}', Clean: ${event.wasClean}`);
                    logMessage(`WebSocket closed. Code: ${event.code}, Reason: ${event.reason || 'No reason specified'}`, 'info');
                    
                    if (isRecording) {
                        console.log("[UI WebSocket]: Unexpected disconnection during recording, stopping recording.");
                        stopRecording();
                    } else {
                        updateStatus('Disconnected from server.');
                    }
                };
            });
        }
    </script>
</body>
</html>