import asyncio
import os
import time
from dataclasses import dataclass
from typing import Any, Dict, Optional

import opencc
from dotenv import load_dotenv
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field

from ..audioinsight.logging_config import get_logger

load_dotenv()

logger = get_logger(__name__)

# Cache OpenCC converter instance to avoid recreation
_s2hk_converter = None


def s2hk(text: str) -> str:
    """Convert Simplified Chinese to Traditional Chinese with cached converter."""
    if not text:
        return text

    global _s2hk_converter
    if _s2hk_converter is None:
        _s2hk_converter = opencc.OpenCC("s2hk")

    return _s2hk_converter.convert(text)


@dataclass
class LLMTrigger:
    """Configuration for when to trigger LLM inference."""

    idle_time_seconds: float = 5.0
    max_text_length: int = 100000
    conversation_trigger_count: int = 2  # Trigger after this many conversations (speaker turns)
    min_text_length: int = 100  # OPTIMIZATION: Minimum text length before considering triggers


class LLMResponse(BaseModel):
    """Structured response from the LLM inference."""

    summary: str = Field(description="Concise summary of the transcription")
    key_points: list[str] = Field(default_factory=list, description="Main points discussed")


class LLM:
    """
    LLM-based transcription processor that monitors transcription activity
    and generates inference after periods of inactivity or after a certain number of conversations.
    """

    def __init__(
        self,
        model_id: str = "openai/gpt-4.1-mini",
        api_key: Optional[str] = None,
        trigger_config: Optional[LLMTrigger] = None,
    ):
        """Initialize the LLM inference processor.

        Args:
            model_id: The model ID to use (defaults to openai/gpt-4.1-mini)
            api_key: Optional API key override (defaults to OPENROUTER_API_KEY env var)
            trigger_config: Configuration for when to trigger LLM inference
        """
        self.model_id = model_id
        self.trigger_config = trigger_config or LLMTrigger()

        # Initialize LLM
        api_key = api_key or os.getenv("OPENROUTER_API_KEY") or os.getenv("OPENAI_API_KEY")
        base_url = "https://openrouter.ai/api/v1" if os.getenv("OPENROUTER_API_KEY") else None

        self.llm = ChatOpenAI(
            model=model_id,
            api_key=api_key,
            base_url=base_url,
        )

        # Create structured LLM for inference responses
        self.structured_llm = self.llm.with_structured_output(LLMResponse, method="function_calling")

        # Create prompt template
        self.prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """You are an expert at summarizing transcriptions from speech-to-text systems.
            
Your task is to analyze the transcription and provide:
1. A concise summary of what was discussed
2. Key points or topics mentioned

Focus on:
- Main topics and themes
- Important decisions or conclusions
- Action items if any
- Overall context and purpose of the conversation

Keep summaries clear and concise while capturing the essential information.

IMPORTANT: Always respond in the same language and script as the transcription. 
- If the transcription is in Chinese (ÁπÅÈ´î‰∏≠Êñá), respond in Traditional Chinese using Hong Kong style conventions.
- If the transcription is in French, respond in French. 
- If it's in English, respond in English. 
- Match the exact language, script, and regional conventions of the input content.""",
                ),
                (
                    "human",
                    """Please summarize this transcription:

Transcription:
{transcription}

Additional context:
- Duration: {duration} seconds
- Has speaker diarization: {has_speakers}
- Number of lines: {num_lines}

Provide a structured summary with key points. Remember to respond in the same language, script, and regional conventions as the transcription above.""",
                ),
            ]
        )

        # State tracking
        self.last_activity_time = time.time()
        self.accumulated_text = ""
        self.last_processed_text = ""  # Keep track of what was already processed
        self.last_inference = None
        self.inference_task = None
        self.is_running = False
        self.inference_callbacks = []
        self.consecutive_idle_checks = 0  # Track consecutive idle periods
        self.min_idle_checks = 3  # Require 3 consecutive idle checks (3 seconds) - reduced from 5 for more responsive summaries

        # Prevent duplicate inference
        self.last_inference_time = 0.0
        self.inference_cooldown = 2.0  # Minimum seconds between inference - reduced from 3.0 for more frequent summaries
        self.is_generating_inference = False  # Flag to prevent concurrent inference

        # Conversation tracking
        self.conversation_count = 0  # Count conversations since last inference
        self.last_speaker = None  # Track the last speaker to detect conversation changes
        self.conversation_count_since_last_inference = 0  # Reset after each inference

        # Statistics
        self.stats = {
            "inference_generated": 0,
            "total_text_processed": 0,
            "average_inference_time": 0.0,
            "inference_by_idle": 0,
            "inference_by_conversation_count": 0,
        }

    def add_inference_callback(self, callback):
        """Add a callback function to be called when inference is generated.

        Args:
            callback: Function that takes (inference_response, transcription_text) as arguments
        """
        self.inference_callbacks.append(callback)

    def add_summary_callback(self, callback):
        """Legacy method for backward compatibility. Use add_inference_callback instead."""
        self.add_inference_callback(callback)

    def update_transcription(self, new_text: str, speaker_info: Optional[Dict] = None):
        """Update with new transcription text.

        Args:
            new_text: New transcription text to add
            speaker_info: Optional speaker/diarization information
        """
        if not new_text.strip():
            return

        self.last_activity_time = time.time()
        self.consecutive_idle_checks = 0  # Reset idle counter on new activity

        # Track conversations (speaker turns)
        current_speaker = speaker_info.get("speaker") if speaker_info and "speaker" in speaker_info else None

        # If speaker changes or this is the first message, it's a new conversation
        if self.last_speaker != current_speaker:
            self.conversation_count_since_last_inference += 1
            self.last_speaker = current_speaker
            logger.debug(f"New conversation detected: {self.conversation_count_since_last_inference} conversations since last inference")

        # Add to accumulated text with speaker info if available
        if speaker_info and "speaker" in speaker_info:
            formatted_text = f"[Speaker {speaker_info['speaker']}]: {new_text}"
        else:
            formatted_text = new_text

        if self.accumulated_text:
            self.accumulated_text += " " + formatted_text
        else:
            self.accumulated_text = formatted_text

        logger.debug(f"Updated transcription: {len(self.accumulated_text)} chars total, " f"conversations: {self.conversation_count_since_last_inference}, idle_checks reset to 0")

    async def start_monitoring(self):
        """Start monitoring for inference triggers."""
        if self.is_running:
            return

        self.is_running = True
        logger.info("Started LLM inference monitoring")

        while self.is_running:
            try:
                await self._check_and_process()
                await asyncio.sleep(1.0)  # Check every second
            except Exception as e:
                logger.error(f"Error in inference monitoring: {e}")
                await asyncio.sleep(5.0)  # Back off on error

    async def stop_monitoring(self):
        """Stop monitoring."""
        self.is_running = False
        logger.info("Stopped LLM inference monitoring")

    async def _check_and_process(self):
        """Check if conditions are met for inference."""
        if not self.accumulated_text.strip():
            self.consecutive_idle_checks = 0
            return

        # Skip check if already generating or in cooldown
        current_time = time.time()
        if self.is_generating_inference or (current_time - self.last_inference_time) < self.inference_cooldown:
            return

        time_since_activity = current_time - self.last_activity_time
        text_length = len(self.accumulated_text)

        # OPTIMIZATION: Skip if we don't have minimum text length yet
        if text_length < self.trigger_config.min_text_length:
            self.consecutive_idle_checks = 0
            return

        # Check if we're truly idle (no new activity for required time)
        if time_since_activity >= 1.0:  # At least 1 second since last activity
            self.consecutive_idle_checks += 1
        else:
            self.consecutive_idle_checks = 0
            # Even if not idle, check for conversation count trigger
            if self.conversation_count_since_last_inference >= self.trigger_config.conversation_trigger_count:
                logger.info(f"üîÑ Triggering inference: {self.conversation_count_since_last_inference} conversations reached (target: {self.trigger_config.conversation_trigger_count})")
                await self._generate_inference(trigger_reason="conversation_count")
            return

        # Check both idle time and conversation count triggers
        new_text_length = len(self.accumulated_text) - len(self.last_processed_text)
        is_truly_idle = self.consecutive_idle_checks >= self.min_idle_checks
        has_enough_conversations = self.conversation_count_since_last_inference >= self.trigger_config.conversation_trigger_count

        # OPTIMIZATION: Increase text length requirements to avoid spam
        accumulated_length = len(self.accumulated_text)
        has_enough_text = accumulated_length > 800 and new_text_length > 300  # Increased thresholds

        logger.debug(f"Trigger check: idle={self.consecutive_idle_checks}s, " f"conversations={self.conversation_count_since_last_inference}, " f"new_text={new_text_length} chars, accumulated={accumulated_length} chars, " f"truly_idle={is_truly_idle}, enough_conversations={has_enough_conversations}, enough_text={has_enough_text}")

        # Trigger inference if any condition is met, prioritizing conversation count > text length > idle
        if has_enough_conversations:
            trigger_reason = "conversation_count"
            logger.info(f"üîÑ Triggering inference: {trigger_reason} trigger - " f"conversations={self.conversation_count_since_last_inference}")
            await self._generate_inference(trigger_reason=trigger_reason)
            self.consecutive_idle_checks = 0  # Reset after processing
        elif has_enough_text:
            trigger_reason = "text_length"
            logger.info(f"üîÑ Triggering inference: {trigger_reason} trigger - " f"accumulated={accumulated_length} chars, new={new_text_length} chars")
            await self._generate_inference(trigger_reason=trigger_reason)
            self.consecutive_idle_checks = 0  # Reset after processing
        elif is_truly_idle:
            trigger_reason = "idle"
            logger.info(f"üîÑ Triggering inference: {trigger_reason} trigger - " f"idle={self.consecutive_idle_checks}s")
            await self._generate_inference(trigger_reason=trigger_reason)
            self.consecutive_idle_checks = 0  # Reset after processing
        elif self.consecutive_idle_checks > 0 and self.consecutive_idle_checks % 5 == 0:
            # OPTIMIZATION: Log every 5 seconds instead of 3 when we're in idle mode
            logger.info(f"‚è≥ Idle for {self.consecutive_idle_checks}s, conversations={self.conversation_count_since_last_inference}, new_text={new_text_length} chars, accumulated={accumulated_length} chars")

    async def _generate_inference(self, trigger_reason: str = "idle"):
        """Generate inference using the LLM.

        Args:
            trigger_reason: Reason for triggering the inference ('idle', 'conversation_count', 'forced', or 'both')
        """
        if not self.accumulated_text.strip():
            return

        # Check if we're in cooldown period or already generating (except for forced inference)
        current_time = time.time()
        if trigger_reason != "forced" and ((current_time - self.last_inference_time) < self.inference_cooldown):
            logger.debug(f"Inference cooldown active: {current_time - self.last_inference_time:.1f}s < {self.inference_cooldown}s")
            return

        if self.is_generating_inference:
            logger.debug("Inference generation already in progress, skipping")
            return

        self.is_generating_inference = True

        try:
            # For forced inference (final summary), process the entire accumulated text
            # For regular inference, get only the new text since last inference
            if trigger_reason == "forced":
                # Process entire accumulated text for comprehensive final summary
                text_to_process = self.accumulated_text.strip()
                logger.info(f"Processing entire accumulated text for final comprehensive summary: {len(text_to_process)} chars")
            else:
                # Regular incremental processing logic
                if self.last_processed_text:
                    # Find where the last processed text ends in the current accumulated text
                    last_inference_end = self.accumulated_text.find(self.last_processed_text)
                    if last_inference_end != -1:
                        last_inference_end += len(self.last_processed_text)
                        text_to_process = self.accumulated_text[last_inference_end:].strip()
                        if not text_to_process:
                            logger.debug("No new text to process")
                            return
                    else:
                        # If we can't find the overlap, process the recent portion
                        text_to_process = self.accumulated_text[-self.trigger_config.max_text_length :].strip()
                else:
                    text_to_process = self.accumulated_text

            # Truncate if too long
            if len(text_to_process) > self.trigger_config.max_text_length:
                text_to_process = text_to_process[-self.trigger_config.max_text_length :]
                logger.info(f"Truncated text to {self.trigger_config.max_text_length} characters")

            start_time = time.time()
            if trigger_reason == "forced":
                logger.info(f"Generating comprehensive final inference for {len(text_to_process)} chars...")
            else:
                logger.info(f"Generating inference for {len(text_to_process)} chars of new content...")

            # Prepare context information
            lines = text_to_process.split("\n")
            has_speakers = "[Speaker" in text_to_process
            duration_estimate = len(text_to_process) / 10  # Rough estimate: 10 chars per second

            # Create chain and invoke
            chain = self.prompt | self.structured_llm
            response: LLMResponse = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: chain.invoke(
                    {
                        "transcription": text_to_process,
                        "duration": duration_estimate,
                        "has_speakers": has_speakers,
                        "num_lines": len(lines),
                    }
                ),
            )

            generation_time = time.time() - start_time

            # Apply s2hk conversion to ensure Traditional Chinese output
            response.summary = s2hk(response.summary)
            response.key_points = [s2hk(point) for point in response.key_points]

            # Update statistics
            self.stats["inference_generated"] += 1
            self.stats["total_text_processed"] += len(text_to_process)

            # Track trigger reason statistics
            if trigger_reason == "idle":
                self.stats["inference_by_idle"] += 1
            elif trigger_reason == "conversation_count":
                self.stats["inference_by_conversation_count"] += 1
            elif trigger_reason == "text_length":
                # Add new stat category for text length trigger
                if "inference_by_text_length" not in self.stats:
                    self.stats["inference_by_text_length"] = 0
                self.stats["inference_by_text_length"] += 1
            elif trigger_reason == "forced":
                # Add new stat category for forced inference
                if "inference_by_forced" not in self.stats:
                    self.stats["inference_by_forced"] = 0
                self.stats["inference_by_forced"] += 1
            else:  # "both"
                self.stats["inference_by_idle"] += 1
                self.stats["inference_by_conversation_count"] += 1

            # Update average time
            prev_avg = self.stats["average_inference_time"]
            count = self.stats["inference_generated"]
            self.stats["average_inference_time"] = (prev_avg * (count - 1) + generation_time) / count

            self.last_inference = response
            self.last_inference_time = current_time  # Update last inference time

            logger.info(f"Generated inference in {generation_time:.2f}s: {len(response.summary)} chars")
            logger.debug(f"Inference: {response.summary}")

            # Call registered callbacks
            for callback in self.inference_callbacks:
                try:
                    await callback(response, text_to_process)
                except Exception as e:
                    logger.error(f"Error in inference callback: {e}")

            # Reset conversation count after processing (except for forced inference to avoid affecting ongoing monitoring)
            if trigger_reason != "forced":
                self.conversation_count_since_last_inference = 0
                logger.debug(f"Reset conversation count to 0 after processing")

            # Update the last processed text to include what we just processed (except for forced inference)
            if trigger_reason != "forced":
                # Keep a rolling buffer to allow for new inferences of additional content
                self.last_processed_text = self.accumulated_text

                # Keep the most recent text in buffer (last 5000 chars) to maintain context
                if len(self.accumulated_text) > 5000:
                    self.accumulated_text = self.accumulated_text[-5000:]
                    self.last_processed_text = self.accumulated_text

        except Exception as e:
            logger.error(f"Failed to generate inference: {e}")
        finally:
            self.is_generating_inference = False

    def get_last_inference(self) -> Optional[LLMResponse]:
        """Get the most recent inference."""
        return self.last_inference

    def get_last_summary(self) -> Optional[LLMResponse]:
        """Legacy method for backward compatibility. Use get_last_inference instead."""
        return self.get_last_inference()

    def get_stats(self) -> Dict[str, Any]:
        """Get inference statistics."""
        return self.stats.copy()

    async def force_inference(self) -> Optional[LLMResponse]:
        """Force generate a inference of current accumulated text."""
        if not self.accumulated_text.strip():
            return None

        await self._generate_inference(trigger_reason="forced")
        return self.last_inference

    async def force_summary(self) -> Optional[LLMResponse]:
        """Legacy method for backward compatibility. Use force_inference instead."""
        return await self.force_inference()
